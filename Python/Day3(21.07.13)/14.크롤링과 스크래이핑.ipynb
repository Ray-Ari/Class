{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513b5268",
   "metadata": {},
   "source": [
    "# 데이터 다운로드 하기 \n",
    "\n",
    ":인터넷에서 지정된 파일을 내 PC에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library\n",
    "import urllib.request\n",
    "\n",
    "#URL과 저장경로 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/test.png\"\n",
    "\n",
    "#다운로드하기\n",
    "urllib.request.urlretrieve(url, savename)\n",
    "print(\"저장 되었습니다.!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ebd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library\n",
    "import urllib.request\n",
    "\n",
    "#URL과 저장경로 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/test1.png\"\n",
    "\n",
    "#다운로드하기\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "#파일을 저장하기\n",
    "with open(savename,\"wb\") as f:\n",
    "    f.write(mem)\n",
    "print(\"저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c1f090",
   "metadata": {},
   "source": [
    "# BeatifulSoup로 Scraping 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a174bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML sample\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1>Header</h1>\n",
    "            <p> Line 1을 서술</p>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "# HTML분석\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(soup)\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "h1 = soup.html.body.h1\n",
    "p = soup.html.body.p\n",
    "print(h1)\n",
    "print(p)\n",
    "print(\"-------\")\n",
    "\n",
    "# Text만 출력\n",
    "print(\"h1=\", h1.string)\n",
    "print(\"h1=\", h1.text)\n",
    "print(\"p=\", p.string)\n",
    "print(\"p=\", p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788373df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id로 요소를 추출하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# html Sample\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1 id=\"title\"> Header</h1>\n",
    "            <p id=\"body\"> Line 1을 서술</p>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "# html 분석\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "title = soup.find(id = 'title')\n",
    "body = soup.find(id = 'body')\n",
    "print(title)\n",
    "print(body)\n",
    "\n",
    "#텍스트만 출력\n",
    "print(\"title=\", title.string)\n",
    "print(\"body=\", body.string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3bbb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러개의 요소 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <ul>\n",
    "              <li><a href=\"http://www.naver.com\"> naver</a></li>\n",
    "              <li><a href=\"http://www.daum.net\"> daum</a></li>\n",
    "            </ul>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# 링크 목록 출력\n",
    "for link in links : \n",
    "#     print(link.string)\n",
    "#     print(link.attrs['href'])\n",
    "    href = link.attrs['href']\n",
    "    text = link.string\n",
    "    print(text, \">>>\", href)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기상청 자료 활용하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "\n",
    "# data 가져오기\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "# print(soup)\n",
    "\n",
    "# 원하는 데이터 추출하기\n",
    "title = soup.find('title').string\n",
    "print(title)\n",
    "wf = soup.find('wf').string\n",
    "\n",
    "for i in wf.split('<br />'):\n",
    "    print(i)\n",
    "    \n",
    "list_wfs = list(wf)\n",
    "except_char = ['<','b','r','/','>']\n",
    "result=\"\"\n",
    "\n",
    "for lwf in list_wfs :\n",
    "    if lwf not in except_char :\n",
    "        result += lwf\n",
    "        \n",
    "print('-'*100)\n",
    "print(result)\n",
    "print(result.split('○'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9446240",
   "metadata": {},
   "source": [
    "# CSS 선택자 사용하기\n",
    "soup.select_one() : css 선택자로 요소 하나를 추출.   \n",
    "soup.elect() : css 선택자로 요소 여러개를 리스트로 추출."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0b9d6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위키 북스\n",
      "유니티 게임 이펙트 입문서\n",
      "스위프트 시작하는 아이폰 앱 개발 교과서\n",
      "모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# html Sample\n",
    "html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div id = \"meigen\">\n",
    "                    <h1>위키 북스</h1>\n",
    "                    <ul class = \"items\">\n",
    "                        <li>유니티 게임 이펙트 입문서</li>\n",
    "                        <li>스위프트 시작하는 아이폰 앱 개발 교과서</li>\n",
    "                        <li>모던 웹사이트 디자인의 정석</li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "# 필요한 부분 CSS로 추출하기\n",
    "# 타이틀 부분 추출하기\n",
    "\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string # 찾는법 id:#, class:, >: 자손, \"  \" : 후손\n",
    "print(h1)\n",
    "\n",
    "# 목록 부분 추출하기\n",
    "li_lists = soup.select(\"div#meigen > ul.items > li\")\n",
    "\n",
    "#print(li_lists)\n",
    "for li in li_lists:\n",
    "    print(li.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6249dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 금융에서 환율 정보 추출하기\n",
    "# https://finance.naver.com/marketindex/\n",
    "# #exchangeList > li.on > a.head.usd > div > span.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "636713fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw :  1,143.30\n"
     ]
    }
   ],
   "source": [
    "# 미국 환율 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 데이터 추출하기\n",
    "price = soup.select_one(\"div.head_info > span.value\").string\n",
    "print(\"usd/krw : \", price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06112d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 :  1,144.00\n",
      "일본 :  1,036.42\n",
      "유럽연합 :  1,358.16\n",
      "중국 :  176.85\n"
     ]
    }
   ],
   "source": [
    "# 미국, 일본, 유럽연합, 중국의 환율\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 데이터 추출하기\n",
    "prices = soup.select(\"div.head_info > span.value\")\n",
    "print(\"미국 : \", prices[0].string)\n",
    "print(\"일본 : \", prices[1].string)\n",
    "print(\"유럽연합 : \", prices[2].string)\n",
    "print(\"중국 : \", prices[3].string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "98cf6ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 하늘과 바람과 별과 시\n",
      "- 서시\n",
      "- 자화상\n",
      "- 소년\n",
      "- 눈 오는 지도\n",
      "- 돌아와 보는 밤\n",
      "- 병원\n",
      "- 새로운 길\n",
      "- 간판 없는 거리\n",
      "- 태초의 아침\n",
      "- 또 태초의 아침\n",
      "- 새벽이 올 때까지\n",
      "- 무서운 시간\n",
      "- 십자가\n",
      "- 바람이 불어\n",
      "- 슬픈 족속\n",
      "- 눈감고 간다\n",
      "- 또 다른 고향\n",
      "- 길\n",
      "- 별 헤는 밤\n",
      "- 흰 그림자\n",
      "- 사랑스런 추억\n",
      "- 흐르는 거리\n",
      "- 쉽게 씌어진 시\n",
      "- 봄\n",
      "- 참회록\n",
      "- 간(肝)\n",
      "- 위로\n",
      "- 팔복\n",
      "- 못자는밤\n",
      "- 달같이\n",
      "- 고추밭\n",
      "- 아우의 인상화\n",
      "- 사랑의 전당\n",
      "- 이적\n",
      "- 비오는 밤\n",
      "- 산골물\n",
      "- 유언\n",
      "- 창\n",
      "- 바다\n",
      "- 비로봉\n",
      "- 산협의 오후\n",
      "- 명상\n",
      "- 소낙비\n",
      "- 한난계\n",
      "- 풍경\n",
      "- 달밤\n",
      "- 장\n",
      "- 밤\n",
      "- 황혼이 바다가 되어\n",
      "- 아침\n",
      "- 빨래\n",
      "- 꿈은 깨어지고\n",
      "- 산림\n",
      "- 이런날\n",
      "- 산상\n",
      "- 양지쪽\n",
      "- 닭\n",
      "- 가슴 1\n",
      "- 가슴 2\n",
      "- 비둘기\n",
      "- 황혼\n",
      "- 남쪽 하늘\n",
      "- 창공\n",
      "- 거리에서\n",
      "- 삶과 죽음\n",
      "- 초한대\n",
      "- 산울림\n",
      "- 해바라기 얼굴\n",
      "- 귀뚜라미와 나와\n",
      "- 애기의 새벽\n",
      "- 햇빛·바람\n",
      "- 반디불\n",
      "- 둘 다\n",
      "- 거짓부리\n",
      "- 눈\n",
      "- 참새\n",
      "- 버선본\n",
      "- 편지\n",
      "- 봄\n",
      "- 무얼 먹구 사나\n",
      "- 굴뚝\n",
      "- 햇비\n",
      "- 빗자루\n",
      "- 기왓장 내외\n",
      "- 오줌싸개 지도\n",
      "- 병아리\n",
      "- 조개껍질\n",
      "- 겨울\n",
      "- 트루게네프의 언덕\n",
      "- 달을 쏘다\n",
      "- 별똥 떨어진 데\n",
      "- 화원에 꽃이 핀다\n",
      "- 종시\n"
     ]
    }
   ],
   "source": [
    "#mw-content-text > div.mw-parser-output > ul:nth-child(6) > li > a\n",
    "#mw-content-text > div.mw-parser-output > ul:nth-child(6) > li > ul > li:nth-child(1) > a\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 데이터 추출하기\n",
    "a_list = soup.select(\"div.mw-parser-output > ul a\")\n",
    "# print(thumb)\n",
    "\n",
    "for a in a_list : \n",
    "    if a.string == \"증보판\" :\n",
    "        continue\n",
    "    print(\"-\",a.string)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f6def",
   "metadata": {},
   "source": [
    "# 다음 영화 연간 순위 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "91458851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1  :  남산의 부장들\n",
      " 2  :  다만 악에서 구하소서\n",
      " 3  :  반도\n",
      " 4  :  히트맨\n",
      " 5  :  테넷\n",
      " 6  :  백두산\n",
      " 7  :  #살아있다\n",
      " 8  :  강철비2: 정상회담\n",
      " 9  :  담보\n",
      "10  :  닥터 두리틀\n",
      "11  :  삼진그룹 영어토익반\n",
      "12  :  정직한 후보\n",
      "13  :  도굴\n",
      "14  :  클로젯\n",
      "15  :  오케이 마담\n",
      "16  :  해치지않아\n",
      "17  :  천문: 하늘에 묻는다\n",
      "18  :  결백\n",
      "19  :  1917\n",
      "20  :  작은 아씨들\n",
      "21  :  미드웨이\n",
      "22  :  시동\n",
      "23  :  지푸라기라도 잡고 싶은 짐승들\n",
      "24  :  미스터 주: 사라진 VIP\n",
      "25  :  인비저블맨\n",
      "26  :  나쁜 녀석들: 포에버\n",
      "27  :  국제수사\n",
      "28  :  침입자\n",
      "29  :  스타워즈: 라이즈 오브 스카이워커\n",
      "30  :  스파이 지니어스 \n",
      "31  :  이웃사촌\n",
      "32  :  온워드: 단 하루의 기적\n",
      "33  :  소리도 없이\n",
      "34  :  버즈 오브 프레이(할리 퀸의 황홀한 해방)\n",
      "35  :  원더 우먼 1984\n",
      "36  :  겨울왕국 2\n",
      "37  :  오! 문희\n",
      "38  :  그린랜드\n",
      "39  :  위대한 쇼맨\n",
      "40  :  런\n",
      "41  :  뮬란\n",
      "42  :  내가 죽던 날\n",
      "43  :  기생충\n",
      "44  :  신비아파트 극장판 하늘도깨비 대 요르문간드\n",
      "45  :  프리즌 이스케이프\n",
      "46  :  검객\n",
      "47  :  조제\n",
      "48  :  사라진 시간\n",
      "49  :  밤쉘: 세상을 바꾼 폭탄선언\n",
      "50  :  알라딘\n"
     ]
    }
   ],
   "source": [
    "#https://movie.daum.net/boxoffice/yearly\n",
    "#mainContent > div > div.box_boxoffice > ol > li:nth-child(1) > div > div.thumb_cont > strong > a\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://movie.daum.net/boxoffice/yearly\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 데이터 추출하기\n",
    "a_list = soup.select(\"strong > a\")\n",
    "\n",
    "count = 1\n",
    "for a in a_list : \n",
    "    print('%2d'%count,\" : \",a.string)\n",
    "    count+=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb4cceb",
   "metadata": {},
   "source": [
    "# 다음 IT News 많이 본 뉴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ceaa93a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://v.daum.net/v/20210713140143407  :  이재명, 긴급 기자회견..\"4차 팬데믹 못막으면 전면봉쇄 불가피\"\n",
      "https://v.daum.net/v/20210713144711324  :  \"일출 보려다 물어뜯길 뻔\"..성산일출봉까지 점령한 야생들개\n",
      "https://v.daum.net/v/20210713124446148  :  \"누나가 무슨 부모야\" 친누나 30차례 찔러 살해한 남동생.. 무기징역 구형\n",
      "https://v.daum.net/v/20210713132549158  :  BJ철구, 7살딸과 '여캠BJ 새엄마 월드컵'..아동학대 논란\n",
      "https://v.daum.net/v/20210713144622300  :  \"확진자인데 와서 미안\" 농담에 카페 영업중단..업무방해 무죄\n",
      "https://v.daum.net/v/20210713133842565  :  정은경 '靑 방역기획관에 밀렸나' 질문에 \"소신껏 하고 있다\" 일축\n",
      "https://v.daum.net/v/20210713114057051  :  \"술집 못가니 호텔 가자\" 60대 유명화가, 20대 성폭행 의혹\n",
      "https://v.daum.net/v/20210713113441712  :  우리은행 본점서 코로나19 집단감염..\"발설 시 엄벌\"\n",
      "https://v.daum.net/v/20210713113230612  :  홍남기, 전국민 지원금 합의에 \"동의 안한다\"..여당과 충돌\n",
      "https://v.daum.net/v/20210713150601168  :  \"철학 붕괴\" 윤희숙 연일 이준석 비판..하태경 \"자해정치\"\n",
      "https://v.daum.net/v/20210713144944406  :  중등교원 양성 규모 축소..사범대 나와야 국영수 교사된다\n",
      "https://v.daum.net/v/20210713135902248  :  \"이 시국에 떼창?\" 팬들 반발에..백기 든 '미스터트롯' 공연\n",
      "https://v.daum.net/v/20210713143600846  :  청와대 앞에 착륙한 미끼 전투기, 힘을 보여주고 싶었다\n",
      "https://v.daum.net/v/20210713115724852  :  \"한국인 DNA에 표현력 없다\" 바이올린 거장 주커만 인종차별\n",
      "https://v.daum.net/v/20210713120602280  :  습도 높으면 35도서도 치명타..습한 한국 여름 무서운 이유\n",
      "https://v.daum.net/v/20210713133106300  :  '천조국 최종병기' 대륙간탄도미사일 미니트맨 Ⅲ\n",
      "https://v.daum.net/v/20210713115624812  :  무려 60년간 조용하던 쿠바인들은 왜 들고 일어났나\n",
      "https://v.daum.net/v/20210713125407426  :  짧고 굵은 장마 19일까지..20일부터 강한 폭염 온다\n",
      "https://v.daum.net/v/20210713131902974  :  \"돼지코 같다\" 싸늘한 반응..BMW 4시리즈 참혹한 결과\n",
      "https://v.daum.net/v/20210713133325380  :  예고 없이 잡힌 미스터트롯 전주 콘서트, 잇단 항의에 취소\n",
      "https://v.daum.net/v/20210713111827741  :  \"살려주세요\" 외침에 바로 '풍덩'..초등생 3명 잇따라 구한 40대\n",
      "https://v.daum.net/v/20210713142508329  :  부산도 위태롭다..유흥시설 넘어 식당·학교 등 전방위 확산\n",
      "https://v.daum.net/v/20210713151947679  :  전여옥, 위기의 이준석에 \"뒤에서 칼꽂는 게 정치..성인식 치러\"\n",
      "https://v.daum.net/v/20210713141248818  :  \"제가 가르쳤고 믿고 있는..\" 조국, 아내 정경심 징역 7년 구형에 '무죄 추정' 대법 판결 공유\n",
      "https://v.daum.net/v/20210713131915980  :  \"4000모 이식한거 맞아?\"..모발 이식 사진은 의료기록인가, 아닌가\n",
      "https://v.daum.net/v/20210713115700827  :  \"내년 하반기 미 금융시장 대폭락 온다\"\n",
      "https://v.daum.net/v/20210713140105375  :  연인 관계 2/3, 친구서 발전\n",
      "https://v.daum.net/v/20210713140846683  :  중·고교 교사 양성 규모 축소한다..사범대 안나오면 국영수 교사 되기 어려워\n",
      "https://v.daum.net/v/20210713132558167  :  정은경, 55~59세 예약 중단에 \"안내 못드려 송구..충분히 접종 가능\"\n",
      "https://v.daum.net/v/20210713151603547  :  양어장을 탈출한 송어는 뇌가 커진다\n"
     ]
    }
   ],
   "source": [
    "#https://media.daum.net/digital/\n",
    "#mAside > div.aside_g.aside_ranking > ul > li.on > div > ol:nth-child(2) > li:nth-child(1) > strong > a\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://news.daum.net/digital\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 데이터 추출하기\n",
    "a_list = soup.select(\".on .tit_g a\")\n",
    "\n",
    "for a in a_list : \n",
    "    href = a.attrs['href']\n",
    "    text = a.string.strip()\n",
    "    print(href, \" : \",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca1224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
